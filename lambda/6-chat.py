import os
import re
import json
import math
import boto3
import traceback
from datetime import datetime, timezone, timedelta
from typing import Optional
from openai import OpenAI

# ====== è¨­å®š ======
S3_BUCKET = os.getenv("S3_BUCKET", "chat-for-vill-reference")
VECTOR_KEY = os.getenv("VECTOR_KEY", "vector/index.jsonl")
CACHE_PREFIX = os.getenv("CACHE_PREFIX", "cache/")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

oa = OpenAI(api_key=OPENAI_API_KEY)
s3 = boto3.client("s3")

_VECTOR_INDEX = None
_CACHE_MAP = None


# ====== Cosineé¡ä¼¼åº¦ ======
def _cosine(a, b):
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb) if na and nb else 0.0


# ====== ãƒ™ã‚¯ãƒˆãƒ«ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­è¾¼ ======
def _load_vector_index():
    global _VECTOR_INDEX
    if _VECTOR_INDEX is not None:
        return _VECTOR_INDEX

    prefix = os.getenv("VECTOR_PREFIX", "vector/")
    print(f"[DEBUG] loading all vector files from s3://{S3_BUCKET}/{prefix}")

    _VECTOR_INDEX = []
    resp = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=prefix)
    if "Contents" not in resp:
        print(f"[WARN] no vector objects found under {prefix}")
        return _VECTOR_INDEX

    for obj in resp["Contents"]:
        key = obj["Key"]
        if not key.endswith(".jsonl"):
            continue
        try:
            body = s3.get_object(Bucket=S3_BUCKET, Key=key)["Body"].read().decode("utf-8")
            for line in body.splitlines():
                if not line.strip():
                    continue
                rec = json.loads(line)
                _VECTOR_INDEX.append(rec)
            print(f"[LOAD] vector file loaded: {key}")
        except Exception as e:
            print(f"[WARN] failed to load {key}: {e}")

    print(f"[LOAD] total vector_index entries={len(_VECTOR_INDEX)}")
    return _VECTOR_INDEX


def _load_cache_map():
    global _CACHE_MAP
    if _CACHE_MAP is not None:
        return _CACHE_MAP

    print(f"[DEBUG] loading all cache files from s3://{S3_BUCKET}/{CACHE_PREFIX}")
    _CACHE_MAP = {}

    resp = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix=CACHE_PREFIX)
    if "Contents" not in resp:
        print(f"[WARN] no cache objects found under {CACHE_PREFIX}")
        return _CACHE_MAP

    for obj in resp["Contents"]:
        key = obj["Key"]
        if not key.endswith(".jsonl"):
            continue

        try:
            body = s3.get_object(Bucket=S3_BUCKET, Key=key)["Body"].read().decode("utf-8")
            for line in body.splitlines():
                if not line.strip():
                    continue
                r = json.loads(line)
                url = r.get("url")
                ci = int(r.get("chunk_index", 0))
                content = r.get("content", "")
                if url and content:
                    _CACHE_MAP[(url, ci)] = content
            print(f"[LOAD] cache file loaded: {key}")
        except Exception as e:
            print(f"[WARN] failed to load {key}: {e}")

    print(f"[LOAD] total cache_map entries={len(_CACHE_MAP)}")
    return _CACHE_MAP

def _detect_year_from_query(query: str):
    """è³ªå•æ–‡ã‹ã‚‰å¯¾è±¡å¹´ï¼ˆè¥¿æš¦ï¼‰ã‚’æ¨å®š"""
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    current_year = now.year

    if m := re.search(r"ä»¤å’Œ\s*(\d+)", query):
        return 2018 + int(m.group(1))
    if m := re.search(r"20\d{2}", query):
        return int(m.group(0))
    if "ä»Šå¹´" in query:
        return current_year
    if "æ˜¨å¹´" in query or "å»å¹´" in query:
        return current_year - 1
    if "æ¥å¹´" in query:
        return current_year + 1
    return current_year


def _detect_year_from_text(text: str):
    """æœ¬æ–‡ãƒ»URLãƒ»ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¹´åº¦ã‚’æ¨å®šï¼ˆä»¤å’Œ or è¥¿æš¦ï¼‰
       URLæœ«å°¾ã‚„ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: /2025/09/file.pdfï¼‰ã‚‚ç¢ºå®Ÿã«å¯¾è±¡ã«å«ã‚ã‚‹"""
    if not text:
        return None

    # ä»¤å’Œ â†’ è¥¿æš¦æ›ç®—
    if m := re.search(r"ä»¤å’Œ\s*(\d+)", text):
        return 2018 + int(m.group(1))

    # è¥¿æš¦ â†’ URLã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚‚å¯¾å¿œï¼ˆ/2025/, _2024, 2023.pdfãªã©ï¼‰
    if m := re.search(r"(20\d{2})", text):
        year = int(m.group(1))
        now_year = datetime.now(timezone(timedelta(hours=9))).year
        if 2000 <= year <= now_year + 1:
            return year

    return None



def _search_from_vector(query, top_k=20):
    """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼æ¤œç´¢ï¼šæœ€æ–°æƒ…å ±ã‚’å„ªå…ˆã—ã¤ã¤ã€HTMLã¨PDFã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãæ‰±ã†"""
    emb_q = oa.embeddings.create(
        model="text-embedding-3-small",
        input=query
    ).data[0].embedding

    index = _load_vector_index()
    query_year = _detect_year_from_query(query)

    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    current_year = now.year

    print(f"[SEARCH] query='{query}' (target_year={query_year})")

    # === 1ï¸âƒ£ å…¨ã‚¹ã‚³ã‚¢ã‚’ã¾ãšç®—å‡ºã—ã¦è¡¨ç¤º ===
    raw_scored = []
    for r in index:
        url = r.get("url", "")
        score = _cosine(emb_q, r["embedding"])
        raw_scored.append((score, url))
    raw_scored.sort(key=lambda x: x[0], reverse=True)

    print("â”€â”€â”€[ RAW COSINE SCORES (TOP 20) ]â”€â”€â”€")
    for i, (s, u) in enumerate(raw_scored[:20], start=1):
        print(f"  {i:02d}. {u[:80]}  score={s:.3f}")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    # === 2ï¸âƒ£ é¡ä¼¼åº¦0.8æœªæº€ã‚’ã‚«ãƒƒãƒˆ ===
    scored = []
    for r in index:
        url = r.get("url", "")
        file_name = os.path.basename(url)
        preview = (r.get("preview") or "") + " " + url + " " + file_name
        score = _cosine(emb_q, r["embedding"])
        if score < 0.80:
            continue

        # å¹´ã‚’æœ¬æ–‡ãƒ»URLãƒ»ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨å®š
        page_year = _detect_year_from_text(preview)
        if page_year is None:
            page_year = current_year

        # ğŸ“† å¹´åº¦è£œæ­£
        diff = page_year - current_year
        if diff == 0:
            score += 0.20
        elif diff == -1:
            score += 0.05
        elif diff <= -2:
            score -= 0.10

        # ğŸ†• postç•ªå·ã«ã‚ˆã‚‹å¾®åŠ ç‚¹
        if m := re.search(r"post-(\d+)", url):
            score += int(m.group(1)) / 2_000_000.0

        # ğŸ“° HTML / ğŸ“„ PDF å„ªå…ˆé †ä½
        if url.lower().endswith(".pdf"):
            score -= 0.05
            if page_year == current_year:
                score += 0.20
            elif page_year > current_year:
                score += 0.30
        else:
            score += 0.10

        # ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´è£œæ­£
        for kw in re.findall(r"[ä¸€-é¾ ã-ã‚“ã‚¡-ãƒ³a-zA-Z0-9]+", query):
            if kw in preview:
                score += 0.03

        scored.append((score, r))

    # === 3ï¸âƒ£ ä¸¦ã¹æ›¿ãˆã¦ä¸Šä½Nã‚’è¿”ã™ ===
    scored.sort(key=lambda x: x[0], reverse=True)
    hits = [r for _, r in scored[:top_k]]

    print(f"[SEARCH] top={len(hits)} results (æœ€æ–°å„ªå…ˆï¼‹0.8cutï¼‹å†ã‚¹ã‚³ã‚¢)")
    for i, (s, r) in enumerate(scored[:5], start=1):
        y = _detect_year_from_text(r.get("url", "") or "")
        print(f"  {i}. {r.get('url')}  year={y}  score={s:.3f}")

    return hits


# ====== ç¾åœ¨æ—¥æ™‚ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åæ˜  ======
def _with_current_date(base_prompt: str) -> str:
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    reiwa = now.year - 2018
    date_str = f"{now.year}å¹´{now.month}æœˆ{now.day}æ—¥ï¼ˆä»¤å’Œ{reiwa}å¹´ï¼‰"
    return (
        base_prompt.strip()
        + f"\n\n# ç¾åœ¨æ—¥æ™‚: {date_str}\n"
        "ã€Œä»Šå¹´ã€ã€Œæ¥å¹´ã€ã€Œæ˜¨å¹´ã€ãªã©ã®è¡¨ç¾ã¯ã€å¿…ãšã“ã®æ—¥ä»˜ã‚’åŸºæº–ã«åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚\n"
        "å¤ã„è³‡æ–™ã‚’å‚ç…§ã™ã‚‹å ´åˆã¯ã€ãã®æ—¨ã‚’æ˜ç¤ºã—ã¦è£œè¶³ã™ã‚‹ã“ã¨ã€‚"
    )


# ====== å›ç­”ç”Ÿæˆ ======
def generate_reply(user_message, config, prompt):
    hits = _search_from_vector(user_message)
    cache = _load_cache_map()

    ctx_blocks, sources = [], []
    for h in hits:
        url = h["url"]
        ci = int(h["chunk_index"])
        content = cache.get((url, ci), "")
        if content:
            ctx_blocks.append(f"URL: {url}\n{content[:1000]}")
            sources.append({"url": url, "chunk_index": ci})
    ctx = "\n\n".join(ctx_blocks) if ctx_blocks else "(é–¢é€£æœ¬æ–‡ãªã—)"

    system_prompt = _with_current_date(
        prompt or "ã‚ãªãŸã¯æ±æˆç€¬æ‘ã®æƒ…å ±ã«åŸºã¥ã„ã¦ç­”ãˆã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
    ) + "\nWebãƒšãƒ¼ã‚¸ã‚’æœ€å„ªå…ˆã«ä½¿ã„ã€PDFã¯æœ€çµ‚æ‰‹æ®µã¨ã—ã¦ã®ã¿åˆ©ç”¨ã™ã‚‹ã“ã¨ã€‚"

    user_prompt = f"è³ªå•: {user_message}\n\nä»¥ä¸‹ã¯æ±æˆç€¬æ‘ã®å…¬å¼ã‚µã‚¤ãƒˆç­‰ã‹ã‚‰ã®è³‡æ–™ã˜ã‚ƒã€‚ã“ã‚Œä»¥å¤–ã‚’æ ¹æ‹ ã«ã—ã¦ã¯ãªã‚‰ã¬ã€‚\n\n{ctx}"

    resp = oa.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0.2,
        max_tokens=800,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    reply = resp.choices[0].message.content.strip()
    print(f"[GEN] reply_len={len(reply)} sources={len(sources)}")
    return reply, sources


# ====== Lambda handler ======
def lambda_handler(event, context):
    try:
        payload = json.loads(event.get("body") or "{}")
        message = payload.get("message", "").strip()
        if not message:
            return {"statusCode": 400, "body": json.dumps({"error": "message required"})}

        config = payload.get("config", {})
        prompt = payload.get("prompt", "")
        options = [{"label": "ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹", "next": "restart"}]

        reply, sources = generate_reply(message, config, prompt)
        return {
            "statusCode": 200,
            "body": json.dumps(
                {
                    "reply": reply, 
                    "sources": sources,
                    "options": options
                    }, 
                    ensure_ascii=False)
        }

    except Exception as e:
        print("âŒ ERROR:", repr(e))
        traceback.print_exc()
        return {
            "statusCode": 500,
            "body": json.dumps({"error": "internal_error", "detail": str(e)}, ensure_ascii=False)
        }
