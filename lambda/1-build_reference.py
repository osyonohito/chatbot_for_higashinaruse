# build_reference.lambda_handler  â€” è¦ä»¶â‘ ã€œâ‘£ æº–æ‹ ã®å®Œæˆç‰ˆ
import requests
from bs4 import BeautifulSoup
import json
import time
import datetime
import pytz
from urllib.parse import urljoin, urlsplit, urlunsplit
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import boto3, os

BASE_URL = os.getenv("BASE_URL", "https://vill.higashinaruse.lg.jp/")
OUT_FILE = "vill_reference.json"
DEPTH = 4
DELAY = 0.3

def make_session():
    s = requests.Session()
    s.headers.update({"User-Agent": "Mozilla/5.0 (compatible; vill-crawler/1.0)"})
    retry = Retry(total=3, backoff_factor=0.5,
                  status_forcelist=[429, 500, 502, 503, 504],
                  allowed_methods=["GET", "HEAD"])
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.mount("http://", HTTPAdapter(max_retries=retry))
    return s

def normalize(url: str) -> str:
    """ã‚¯ã‚¨ãƒªã¯åŸå‰‡ã™ã¹ã¦å‰Šé™¤ãƒ»ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã€æœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€ï¼ˆPDFã¯é™¤ãï¼‰"""
    s = urlsplit(url)
    path = s.path
    if not path.endswith(".pdf"):
        if path and path != "/" and not path.endswith("/"):
            path += "/"
    # ã‚¯ã‚¨ãƒªã¯ç©ºã«ã€ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚‚ç©ºã«
    return urlunsplit((s.scheme, s.netloc, path, "", ""))

# â‘  sitemap.xmlã‹ã‚‰URLã‚’ã™ã¹ã¦å–å¾—
def get_all_sitemap_urls(session):
    print("ğŸ“„ Collecting URLs from sitemap.xml ...")
    urls = set()
    idx_url = BASE_URL + "sitemap.xml"

    res = session.get(idx_url, timeout=15)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, "html.parser")

    # --- ã‚µãƒ–ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ä¸€è¦§ã‚’å–å¾— ---
    submaps = [loc.text.strip() for loc in soup.find_all("loc") if loc.text]
    print(f"ğŸ—ºï¸ Found {len(submaps)} sub-sitemaps")

    for sm in submaps:
        if not sm.startswith(BASE_URL):
            continue
        try:
            r = session.get(sm, timeout=15)
            r.encoding = "utf-8"
            subsoup = BeautifulSoup(r.text, "html.parser")

            # å„ã‚µãƒ–ãƒãƒƒãƒ—ã® <loc> ã‚’æŠ½å‡º
            for loc in subsoup.find_all("loc"):
                u = loc.text.strip()
                if u.startswith(BASE_URL):
                    urls.add(normalize(u))
        except Exception as e:
            print(f"âš ï¸ Failed to fetch {sm}: {e}")
            continue

    print(f"âœ… Sitemap URLs collected: {len(urls)}")
    return urls


# â‘¡ å®Ÿéš›ã«è¾¿ã‚Œã‚‹URLã‚’å–å¾— (x) ï¼ï¼ˆã‚µã‚¤ãƒˆãƒãƒƒãƒ—URLé›†åˆ âˆ© å®Ÿåˆ°é”é›†åˆï¼‰
def crawl_reachable_urls_within_sitemap(session, start_url, allowed_set, max_depth=4, delay=0.3):
    visited, to_visit, reachable = set(), {normalize(start_url)}, set()

    for depth in range(max_depth):
        print(f"ğŸŒ¿ Depth {depth+1}/{max_depth} - {len(to_visit)} URLs to crawl")
        new_urls = set()

        for url in to_visit:
            if url in visited or not url.startswith(BASE_URL):
                continue
            visited.add(url)
            try:
                res = session.get(url, timeout=15, allow_redirects=True)
                ctype = (res.headers.get("Content-Type") or "").lower()
                if "text/html" not in ctype:
                    # HTMLä»¥å¤–ï¼ˆç›´PDFãªã©ï¼‰ã¯ (x) ã«ã¯å…¥ã‚Œãªã„ï¼ˆyã§æ‹¾ã†ï¼‰
                    time.sleep(delay)
                    continue

                # â† ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã«è¼‰ã£ã¦ã„ã‚‹URLã ã‘ (x) å€™è£œã«æ¡ç”¨
                nurl = normalize(url)
                if nurl in allowed_set:
                    reachable.add(nurl)

                soup = BeautifulSoup(res.text, "html.parser")
                for a in soup.find_all("a", href=True):
                    abs_url = normalize(urljoin(url, a["href"].strip()))
                    if abs_url.startswith(BASE_URL):
                        new_urls.add(abs_url)

                time.sleep(delay)
            except Exception:
                continue

        to_visit = new_urls - visited

    print(f"âœ… Reachable URLs (within sitemap): {len(reachable)}")
    return reachable

# â‘¢ å®Ÿåˆ°é”URL (x) ã®å„ãƒšãƒ¼ã‚¸ã‹ã‚‰PDFãƒªãƒ³ã‚¯ã‚’æŠ½å‡º (y)
def extract_pdf_links_from_pages(session, pages):
    pdfs = set()
    print("ğŸ“‘ Extracting PDF links from (x) pages...")
    total = len(pages)
    for i, url in enumerate(pages, 1):
        try:
            res = session.get(url, timeout=15)
            ctype = (res.headers.get("Content-Type") or "").lower()
            if "text/html" not in ctype:
                continue
            soup = BeautifulSoup(res.text, "html.parser")
            for a in soup.find_all("a", href=True):
                abs_url = normalize(urljoin(url, a["href"].strip()))
                if abs_url.startswith(BASE_URL) and abs_url.lower().endswith(".pdf"):
                    pdfs.add(abs_url)
        except Exception:
            pass
        if i % 25 == 0:
            print(f"  ... scanned {i}/{total} pages")
        time.sleep(DELAY)
    print(f"âœ… PDF links found: {len(pdfs)}")
    return pdfs

def lambda_handler(event=None, context=None):
    started = datetime.datetime.now(pytz.timezone("Asia/Tokyo"))

    session = make_session()

    # Stepâ‘ : ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®æ­£è¦URLå…¨é›†åˆ
    sitemap_urls = get_all_sitemap_urls(session)

    # Stepâ‘¡: ãƒˆãƒƒãƒ—ã‚’èµ·ç‚¹ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¤ã¤ã€(x) = ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å†…ã«é™å®šã—ã¦å®Ÿåˆ°é”URLã ã‘åé›†
    x_pages = crawl_reachable_urls_within_sitemap(
        session, BASE_URL, allowed_set=sitemap_urls, max_depth=DEPTH, delay=DELAY
    )

    # Stepâ‘¢: (x) ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰è¦‹ãˆã‚‹PDFã‚’æŠ½å‡ºï¼ˆPDFè‡ªä½“ã¯ã‚µã‚¤ãƒˆãƒãƒƒãƒ—å¤–ã§ã‚‚OKï¼‰
    y_pdfs = extract_pdf_links_from_pages(session, sorted(x_pages))

    # Stepâ‘£: (x) âˆª (y) ã‚’å‡ºåŠ›
    unified = sorted(x_pages.union(y_pdfs))
    data = {"base_url": BASE_URL, "total": len(unified), "links": unified}

    s3 = boto3.client("s3")
    bucket = os.getenv("BUCKET_NAME", "chat-for-vill-reference")
    key = os.getenv("REFERENCE_PREFIX", "reference/") + "vill_reference.json"
    s3.put_object(
        Bucket=bucket,
        Key=key,
        Body=json.dumps(data, ensure_ascii=False, indent=2),
        ContentType="application/json"
    )

    finished = datetime.datetime.now(pytz.timezone("Asia/Tokyo"))
    elapsed = (finished - started).total_seconds()
    print(f"ğŸŒ Total links (HTML + PDF): {len(unified)}")
    print(f"â± Elapsed: {elapsed:.1f}s")
    print(f"âœ… Saved to S3: s3://{bucket}/reference/vill_reference.json")

    # âœ… Stepâ‘¤: build_cache_dispatcher ã‚’å‘¼ã³å‡ºã—ï¼ˆéåŒæœŸï¼‰
    try:
        lambda_client = boto3.client("lambda")
        dispatcher_arn = os.getenv("DISPATCHER_LAMBDA_ARN")
        if dispatcher_arn:
            lambda_client.invoke(
                FunctionName=dispatcher_arn,
                InvocationType="Event",  # éåŒæœŸå®Ÿè¡Œ
                Payload=json.dumps({"trigger": "from_reference"})
            )
            print(f"ğŸš€ Triggered build_cache_dispatcher ({dispatcher_arn})")
        else:
            print("âš ï¸ DISPATCHER_LAMBDA_ARN not set, skipping dispatcher invoke")
    except Exception as e:
        print(f"âš ï¸ Failed to trigger dispatcher: {e}")

    # âœ… Lambdaãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®æ­£å¼ãªæˆ»ã‚Šå€¤
    return {
        "statusCode": 200,
        "body": json.dumps({
            "message": "vill_reference.json generated successfully",
            "total": len(unified),
            "elapsed_sec": elapsed
        }, ensure_ascii=False)
    }